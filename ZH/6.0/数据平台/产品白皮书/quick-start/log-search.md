# 无需登录服务器收集全量日志

传统的数据收集，运维需要登录服务器，下载安装采集器，再编写采集配置，这样简单的流程在百台服务之内，还是人力可以控制的范围，但是在大数据时代中，数据具有“3V”特性：Volume（海量性）、Velocity（实时性）、Variety（多样性）这三个特性，也是大数据平台必须要解决的三个主要问题，其中数据收集阶段尤为如此，如何能够不登录服务器收集全量数据呢？

本节会讲解从【新增数据源】->【数据清洗】->【数据入库】->【数据查询】的全流程数据收集与存储案例。

### 第一步：新增数据源

1.在数据平台导航中点击【数据集成】，会展现了用户在数据平台中有权限的已经存在的数据源，用户根据不同维度，像业务、数据来源和数据源分类等，过滤选择数据源。

![-w1889](media/15873528160989.jpg)

如果在数据源列表中没有用户想要的数据源，点击【新接入数据源】，可以增加新的数据源。

![-w1876](media/15873539184689.jpg)


2.选择接入类型，由于本次案例是收集服务器的日志数据，所以选择【日志文件】，点击不同的接入类型，会有不同的数据信息需要用户填写，如图所示，首先需要填写"数据定义"，他主要是数据的元信息，包括：业务归属、数据来源、数据源分类、接入渠道、数据源名称（英文名和中文名）、数据源描述和字符集编码。

3.填写接入对象，不同接入类型对应有不同的接入对象，在【日志文件】场景中，接入对象有两类：IP 和模块（蓝鲸配置平台的模块概念），IP 是指直接填写服务的 IP，如图所示。

![-w898](media/15873540583004.jpg)

选择模块则不需要填写具体的 IP，数据平台会根据蓝鲸配置平台中模块下的 IP 列表下发采集器，同时每 2 分钟更新模块下最新的 IP 列表，对于新增的 IP 会自动部署，如果是删除的 IP 会取消采集。

![-w903](media/15873541081127.jpg)


4.填写接入方式，日志采集的接入方式目前有两种，对于实时产生的流日志，建议使用增量的方式采集

5.设置过滤条件（非必填项），采集日志数据过程中，可以根据分隔符（竖线、逗号、反引号和换行符），过滤行日志数据，并指出过滤条件，注意：建议不要填写比较复杂的过滤条件，一则会影响采集实时性，二则会对服务器性能产生影响。

6.设置数据权限，数据权限的详细介绍可以参考【权限管理】，在新增数据源时，需要指定数据管理员（默认同步业务的运维和运营规划），主要职责是数据权限的审批（包括由此数据源产生的结果数据表）；另一个是需要指定数据敏感度，数据平台规划了四级数据敏感级别，目前开放出来的是业务私有数据，即本业务人员使用数据无需申请，业务之外的人员查询数据需要向数据管理员申请。


7.提交数据源接入配置，可以在数据详情中看到数据接入状态和接入对象列表，如图所示，不同颜色表示不同状态（绿色是成功，红色是异常，蓝色是运行中），也可以最新的原始数据内容，包括分钟的趋势和天级别的趋势

![-w1882](media/15873542701100.jpg)

![-w1356](media/15873546917208.jpg)



### 第二步：数据源清洗

为什么需要数据清洗呢？因为原始日志数据大多数是非结构化或者是半结构化的，如果需要后续的统计计算或者存储到关系型存储中，则需要进行结构化。数据源清洗大致分为四部分：

1.提供原始数据内容，默认情况下，如果接入状态是正常的，则系统会自动提取最新的一条原始数据内容，如果数据没有立即产生，用户可以自己填写一条原始数据。

![-w1272](media/15873547601837.jpg)

![-w1908](media/15873548724693.jpg)


2.清洗逻辑配置，对原始数据一步步的配置清洗算子，将原始数据中需要的内容赋值给指定的字段，清洗算子的使用方法可以参考【清洗算子指南】，每一次的清洗运算都是可以进行单步调试的。

![-w1308](media/15873550124797.jpg)

3.全局调试，在写好全部的清洗算子配置后，可以点击【全局调试】，在结果预览中可以看到原始数据经过全部清洗算子后的结果数据（结构化数据）

![-w1918](media/15873552870837.jpg)

4.在全局调试成功后，则需要定义结构化的结果数据，包括清洗结果名称，清洗之后的字段名称等，**注意：要在字段中选取一个时间字段，便于后续的统计分析**。

数据清洗启动后，可以在清洗列表中查看清洗详情，还可以看到清洗的趋势和清洗数据的丢失情况，以及清洗数据之后关联的计算任务（一般是实时计算任务）。

![-w1270](media/15873554132689.jpg)

![-w1279](media/15873554908508.jpg)

### 第三步：数据入库存储

数据源日志在接入和清洗之后，已经可以进行计算和分析使用了，但是在数据还没有落地，是不能被检索和查询的，所以收集的日志一定要入库保存，便于后面的应用，一般日志都有关键词检索的需求，所以都会保存在 ElasticSearch 中，同时还可以指定字段是否分词等。

![-w1255](media/15873642571097.jpg)

### 第四步：日志关键词检索

日志已经都收集到了 ElasticSearch 中，如果需要检索相关的内容，需要点击【数据查询】，并选择业务，搜索需要检索的结果数据表名，输入关键字，可以选择检索的时间范围（近 7 天，近 30 天等），对于检索需要的规则请参考【检索规则】。

![-w1902](media/15873563058512.jpg)



结束语：至此服务器上的日志已经全量收集完毕，用户无需登录服务器编写脚本，管理采集状态或者更新采集项等，建议可以进入 [10 分钟玩转大数据开发](./dataflow.md)。












